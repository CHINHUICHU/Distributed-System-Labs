# Lab 4A Implementation Notes

## Overview

A fault-tolerant key/value service (Get / Put / Append) layered on top of the Raft
library in `../raft/`. Part A uses no snapshots (`maxraftstate == -1`).

---

## Files changed

| File | What changed |
|------|-------------|
| `common.go` | Added `ClientId int64`, `SeqNum int` to both arg structs |
| `client.go` | Full Clerk implementation with retry + dedup |
| `server.go` | Full KVServer implementation |
| `../raft/raft.go` | Added `replicateCh []chan struct{}` field + initialisation |
| `../raft/agreement.go` | `Start()` signals `replicateCh`; `peerReplicationLoop` uses `select` |

---

## client.go

### Clerk fields
```go
type Clerk struct {
    servers  []*labrpc.ClientEnd
    leaderId int    // last known leader; avoids scanning all servers every RPC
    clientId int64  // unique per Clerk, generated by nrand() once
    seqNum   int    // next sequence number; starts at 1
}
```

### Retry protocol
- `seqNum` is incremented **only** on confirmed success (`ok && reply.Err == OK`).
- A retry re-uses the same `seqNum`, so the server can detect and discard duplicates.
- On failure (dropped RPC or `ErrWrongLeader`) the Clerk advances `leaderId` round-robin.

---

## server.go

### Op struct
```go
type Op struct {
    Type     string // "Get", "Put", "Append"
    Key      string
    Value    string
    ClientId int64
    SeqNum   int
}
```
All fields exported so `labgob` can encode them for Raft log entries.

### Dedup state
```go
lastSeq   map[int64]int    // per-client: highest applied SeqNum
lastReply map[int64]string // per-client: return value for lastSeq entry
```
Only the *most recent* result per client is kept.  Because each client makes one
call at a time, a new `SeqNum > lastSeq` means the previous result is no longer needed.

### notifyCh map
```go
notifyCh map[int]chan notifyMsg // keyed by Raft log index
```
Each RPC handler that submits a command via `rf.Start()` registers a **buffered (size 1)**
channel at the returned log index.  The `applier` goroutine sends on that channel when
the entry commits, and deletes it.

Buffering prevents the applier from blocking if the handler already timed out and removed
the channel reference.

### Handler fast-path (critical correctness fix)
```go
kv.mu.Lock()
if kv.lastSeq[args.ClientId] >= args.SeqNum {
    reply.Err = OK
    reply.Value = kv.lastReply[args.ClientId]  // omitted for PutAppend
    kv.mu.Unlock()
    return
}
// ... call rf.Start() ...
```
**Why this is necessary**: Raft's `Start()` keeps a `seen` map keyed by command value.
When a client retries an already-committed op (e.g., because the reply RPC was dropped),
`Start()` detects the identical `Op` struct in `seen` and returns the **same log index** that
was committed in the previous attempt.  But `applier` has already processed that index
and will never re-send on `notifyCh[index]`, so the handler would wait 500 ms, return
`ErrWrongLeader`, and the client would retry forever.

The fast-path check short-circuits this: if `lastSeq >= seqNum` the op is already applied,
so the handler returns the cached result immediately without touching Raft.

### Handler wait loop
```go
select {
case msg := <-ch:
    // verify (clientId, seqNum) matches — protects against index hijack
    if msg.clientId == args.ClientId && msg.seqNum == args.SeqNum {
        reply.Err = msg.err
        reply.Value = msg.value
    } else {
        reply.Err = ErrWrongLeader
    }
case <-time.After(500 * time.Millisecond):
    reply.Err = ErrWrongLeader
    kv.mu.Lock()
    delete(kv.notifyCh, index)
    kv.mu.Unlock()
}
```
The identity check guards against **index hijacking**: if this server lost leadership and a
new leader committed a *different* op at the same log index, the channel receives a message
but with a different `(clientId, seqNum)`.  We return `ErrWrongLeader` so the Clerk retries.

### applier goroutine
Runs forever reading `kv.applyCh`:
1. Cast `msg.Command` to `Op`.
2. Under `kv.mu`: if `lastSeq[clientId] >= seqNum` use cached result; otherwise execute and update `lastSeq`/`lastReply`.
3. If `notifyCh[index]` exists, send result and delete the channel.

`SnapshotValid` messages are silently ignored in Part A.

---

# Lab 4B Implementation Notes

## Overview

Part B adds snapshotting so the Raft log does not grow without bound.  When
`persister.RaftStateSize() >= maxraftstate` the KV server encodes its full
state into a GOB snapshot and hands it to Raft, which discards the corresponding
log prefix.  On restart (or when a lagging follower receives an InstallSnapshot
RPC) the snapshot is delivered back to the server via `applyCh` and the
in-memory state is restored from it.

Only `server.go` was changed.

---

## New struct fields

```go
persister   *raft.Persister  // to query RaftStateSize()
lastApplied int              // highest Raft index applied; passed to rf.Snapshot()
```

`persister` is stored in `StartKVServer` before `raft.Make` is called.

---

## Snapshot encoding format

Three GOB values written in order by `takeSnapshot` and read in the same order
by `installSnapshot`:

| # | Field | Type |
|---|-------|------|
| 1 | `kv.store` | `map[string]string` |
| 2 | `kv.lastSeq` | `map[int64]int` |
| 3 | `kv.lastReply` | `map[int64]string` |

All three must be saved so that deduplication works correctly after a restart —
without `lastSeq`/`lastReply`, a restarted server would re-execute already-applied
ops that the client retries.

---

## takeSnapshot

```go
func (kv *KVServer) takeSnapshot(index int) {
    w := new(bytes.Buffer)
    e := labgob.NewEncoder(w)
    e.Encode(kv.store)
    e.Encode(kv.lastSeq)
    e.Encode(kv.lastReply)
    kv.rf.Snapshot(index, w.Bytes())
}
```

Called **inline** in the applier while holding `kv.mu`.  `rf.Snapshot` is safe
to call here because Raft's applier releases `rf.mu` before sending to `applyCh`,
so there is no lock-cycle.

---

## installSnapshot

```go
func (kv *KVServer) installSnapshot(data []byte, index int) {
    // decode store, lastSeq, lastReply ...
    kv.store     = store
    kv.lastSeq   = lastSeq
    kv.lastReply = lastReply
    kv.lastApplied = index
    for idx, ch := range kv.notifyCh {
        if idx <= index {
            ch <- notifyMsg{err: ErrWrongLeader}
            delete(kv.notifyCh, idx)
        }
    }
}
```

Called while holding `kv.mu` for both the startup case and the mid-run
InstallSnapshot case.

The loop over `notifyCh` handles the race where a handler submitted an op via
`rf.Start()` and registered a channel, but the log entries it was waiting on are
now covered by the incoming snapshot and will never arrive as `CommandValid`
messages.  Sending `ErrWrongLeader` on those channels causes the RPC handler to
return immediately and the client to retry on a different server.

---

## applier changes

```go
// SnapshotValid branch (new)
if msg.SnapshotValid {
    kv.mu.Lock()
    kv.installSnapshot(msg.Snapshot, msg.SnapshotIndex)
    kv.mu.Unlock()
    continue
}

// after notifyCh send (new)
kv.lastApplied = index
if kv.maxraftstate > 0 && kv.persister.RaftStateSize() >= kv.maxraftstate {
    kv.takeSnapshot(index)
}
```

The snapshot trigger uses `>= maxraftstate` (not a softer fraction) to satisfy
`TestSnapshotSize3B`, which asserts the log stays under `8 × maxraftstate`.

Startup state restoration requires no special code in `StartKVServer`: Raft
automatically delivers the persisted snapshot as a `SnapshotValid` ApplyMsg
before any log entries, so `installSnapshot` handles it uniformly.

---

## Test results (all with `-race`)

```
TestSnapshotRPC3B                                          PASS   3 s
TestSnapshotSize3B                                         PASS  13 s
TestSpeed3B                                                PASS  16 s
TestSnapshotRecover3B                                      PASS  19 s
TestSnapshotRecoverManyClients3B                           PASS  20 s
TestSnapshotUnreliable3B                                   PASS  16 s
TestSnapshotUnreliableRecover3B                            PASS  20 s
TestSnapshotUnreliableRecoverConcurrentPartition3B         PASS  28 s
TestSnapshotUnreliableRecoverConcurrentPartitionLinearizable3B  PASS  30 s
                                                   Total  166 s
```

All 16 Part A tests continue to pass after the Part B changes.

---

## Raft changes (performance fix)

### Problem
`peerReplicationLoop` slept for the full `AppendInterval = 50 ms` after each successful
replication round (when the peer was caught up).  A new entry added by `Start()` would
not be sent until the sleep expired, giving **~50 ms latency per op** — too slow for
`TestSpeed3A` which requires < 33 ms/op.

### Fix: `replicateCh`
```go
// raft.go — Raft struct
replicateCh []chan struct{} // one buffered(1) channel per peer

// agreement.go — Start(), in the "new entry" branch
for i, ch := range rf.replicateCh {
    if i != rf.me {
        select {
        case ch <- struct{}{}:
        default: // already signaled, skip
        }
    }
}

// agreement.go — peerReplicationLoop, "caught up" path
select {
case <-rf.replicateCh[peer]:  // new entry arrived, wake immediately
case <-time.After(AppendInterval): // fallback heartbeat
}
```

This reduces per-op latency from ~50 ms to ~0 ms (limited only by the 10 ms polling
intervals of `checkCommitIndex` and `applier` in raft.go).

---

## Test results (all with `-race`)

```
TestBasic3A                                        PASS  15 s
TestSpeed3A                                        PASS  20 s   (threshold 33 ms/op)
TestConcurrent3A                                   PASS  16 s
TestUnreliable3A                                   PASS  16 s
TestUnreliableOneKey3A                             PASS   1 s
TestOnePartition3A                                 PASS   1 s
TestManyPartitionsOneClient3A                      PASS  23 s
TestManyPartitionsManyClients3A                    PASS  23 s
TestPersistOneClient3A                             PASS  19 s
TestPersistConcurrent3A                            PASS  20 s
TestPersistConcurrentUnreliable3A                  PASS  21 s
TestPersistPartition3A                             PASS  28 s
TestPersistPartitionUnreliable3A                   PASS  28 s
TestPersistPartitionUnreliableLinearizable3A       PASS  31 s
                                          Total   263 s
```
