# Lab 4A Implementation Notes

## Overview

A fault-tolerant key/value service (Get / Put / Append) layered on top of the Raft
library in `../raft/`. Part A uses no snapshots (`maxraftstate == -1`).

---

## Files changed

| File | What changed |
|------|-------------|
| `common.go` | Added `ClientId int64`, `SeqNum int` to both arg structs |
| `client.go` | Full Clerk implementation with retry + dedup |
| `server.go` | Full KVServer implementation |
| `../raft/raft.go` | Added `replicateCh []chan struct{}` field + initialisation |
| `../raft/agreement.go` | `Start()` signals `replicateCh`; `peerReplicationLoop` uses `select` |

---

## client.go

### Clerk fields
```go
type Clerk struct {
    servers  []*labrpc.ClientEnd
    leaderId int    // last known leader; avoids scanning all servers every RPC
    clientId int64  // unique per Clerk, generated by nrand() once
    seqNum   int    // next sequence number; starts at 1
}
```

### Retry protocol
- `seqNum` is incremented **only** on confirmed success (`ok && reply.Err == OK`).
- A retry re-uses the same `seqNum`, so the server can detect and discard duplicates.
- On failure (dropped RPC or `ErrWrongLeader`) the Clerk advances `leaderId` round-robin.

---

## server.go

### Op struct
```go
type Op struct {
    Type     string // "Get", "Put", "Append"
    Key      string
    Value    string
    ClientId int64
    SeqNum   int
}
```
All fields exported so `labgob` can encode them for Raft log entries.

### Dedup state
```go
lastSeq   map[int64]int    // per-client: highest applied SeqNum
lastReply map[int64]string // per-client: return value for lastSeq entry
```
Only the *most recent* result per client is kept.  Because each client makes one
call at a time, a new `SeqNum > lastSeq` means the previous result is no longer needed.

### notifyCh map
```go
notifyCh map[int]chan notifyMsg // keyed by Raft log index
```
Each RPC handler that submits a command via `rf.Start()` registers a **buffered (size 1)**
channel at the returned log index.  The `applier` goroutine sends on that channel when
the entry commits, and deletes it.

Buffering prevents the applier from blocking if the handler already timed out and removed
the channel reference.

### Handler fast-path (critical correctness fix)
```go
kv.mu.Lock()
if kv.lastSeq[args.ClientId] >= args.SeqNum {
    reply.Err = OK
    reply.Value = kv.lastReply[args.ClientId]  // omitted for PutAppend
    kv.mu.Unlock()
    return
}
// ... call rf.Start() ...
```
**Why this is necessary**: Raft's `Start()` keeps a `seen` map keyed by command value.
When a client retries an already-committed op (e.g., because the reply RPC was dropped),
`Start()` detects the identical `Op` struct in `seen` and returns the **same log index** that
was committed in the previous attempt.  But `applier` has already processed that index
and will never re-send on `notifyCh[index]`, so the handler would wait 500 ms, return
`ErrWrongLeader`, and the client would retry forever.

The fast-path check short-circuits this: if `lastSeq >= seqNum` the op is already applied,
so the handler returns the cached result immediately without touching Raft.

### Handler wait loop
```go
select {
case msg := <-ch:
    // verify (clientId, seqNum) matches — protects against index hijack
    if msg.clientId == args.ClientId && msg.seqNum == args.SeqNum {
        reply.Err = msg.err
        reply.Value = msg.value
    } else {
        reply.Err = ErrWrongLeader
    }
case <-time.After(500 * time.Millisecond):
    reply.Err = ErrWrongLeader
    kv.mu.Lock()
    delete(kv.notifyCh, index)
    kv.mu.Unlock()
}
```
The identity check guards against **index hijacking**: if this server lost leadership and a
new leader committed a *different* op at the same log index, the channel receives a message
but with a different `(clientId, seqNum)`.  We return `ErrWrongLeader` so the Clerk retries.

### applier goroutine
Runs forever reading `kv.applyCh`:
1. Cast `msg.Command` to `Op`.
2. Under `kv.mu`: if `lastSeq[clientId] >= seqNum` use cached result; otherwise execute and update `lastSeq`/`lastReply`.
3. If `notifyCh[index]` exists, send result and delete the channel.

`SnapshotValid` messages are silently ignored in Part A.

---

## Raft changes (performance fix)

### Problem
`peerReplicationLoop` slept for the full `AppendInterval = 50 ms` after each successful
replication round (when the peer was caught up).  A new entry added by `Start()` would
not be sent until the sleep expired, giving **~50 ms latency per op** — too slow for
`TestSpeed3A` which requires < 33 ms/op.

### Fix: `replicateCh`
```go
// raft.go — Raft struct
replicateCh []chan struct{} // one buffered(1) channel per peer

// agreement.go — Start(), in the "new entry" branch
for i, ch := range rf.replicateCh {
    if i != rf.me {
        select {
        case ch <- struct{}{}:
        default: // already signaled, skip
        }
    }
}

// agreement.go — peerReplicationLoop, "caught up" path
select {
case <-rf.replicateCh[peer]:  // new entry arrived, wake immediately
case <-time.After(AppendInterval): // fallback heartbeat
}
```

This reduces per-op latency from ~50 ms to ~0 ms (limited only by the 10 ms polling
intervals of `checkCommitIndex` and `applier` in raft.go).

---

## Test results (all with `-race`)

```
TestBasic3A                                        PASS  15 s
TestSpeed3A                                        PASS  20 s   (threshold 33 ms/op)
TestConcurrent3A                                   PASS  16 s
TestUnreliable3A                                   PASS  16 s
TestUnreliableOneKey3A                             PASS   1 s
TestOnePartition3A                                 PASS   1 s
TestManyPartitionsOneClient3A                      PASS  23 s
TestManyPartitionsManyClients3A                    PASS  23 s
TestPersistOneClient3A                             PASS  19 s
TestPersistConcurrent3A                            PASS  20 s
TestPersistConcurrentUnreliable3A                  PASS  21 s
TestPersistPartition3A                             PASS  28 s
TestPersistPartitionUnreliable3A                   PASS  28 s
TestPersistPartitionUnreliableLinearizable3A       PASS  31 s
                                          Total   263 s
```
